{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing Example\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OLEKSANDRRomanko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "try:\n",
    "    from gensim.models import word2vec\n",
    "except:\n",
    "    !pip install gensim\n",
    "    from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "We have 5 examples of documents (tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "'All bears are lovely',\n",
    "'Our tea was bad',\n",
    "'That bear drinks with bear',\n",
    "'The bear drinks tea',\n",
    "'We love bears'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1: \"All bears are lovely\"\n",
      "Example #2: \"Our tea was bad\"\n",
      "Example #3: \"That bear drinks with bear\"\n",
      "Example #4: \"The bear drinks tea\"\n",
      "Example #5: \"We love bears\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print('Example #{0:d}: \"{1:s}\"'.format(i+1,corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "Convert to lower case, remove stop words, stem words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lc = []\n",
    "s1 = ' '\n",
    "corpus_clean = []\n",
    "for line in corpus:\n",
    "    lower_case = line.lower() # lowercase \n",
    "    list_lc.append(lower_case)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')         \n",
    "    b = tokenizer.tokenize(lower_case)\n",
    "    words_rmStop = [word for word in b if word not in stopwords.words('english')] # remove stop words\n",
    "    ps = PorterStemmer()\n",
    "    words_stem = [ps.stem(word) for word in words_rmStop] # stem \n",
    "    corpus_clean.append(s1.join(words_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned example #1: \"bear love\"\n",
      "Cleaned example #2: \"tea bad\"\n",
      "Cleaned example #3: \"bear drink bear\"\n",
      "Cleaned example #4: \"bear drink tea\"\n",
      "Cleaned example #5: \"love bear\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print('Cleaned example #{0:d}: \"{1:s}\"'.format(i+1,corpus_clean[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency (WF)\n",
    "\n",
    "The \"word frequency\" (WF) method records the number of times that term occurs in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0 1 0 1 0]\n",
      " [1 0 0 0 1]\n",
      " [0 2 1 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "array_WF = vectorizer.fit_transform(corpus_clean).toarray()\n",
    "#print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(array_WF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"bag of words\" (WF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tbad    bear   drink  love   tea    \t\n",
      "Example #1      0      1      0      1      0         \"bear love\"\t\n",
      "Example #2      1      0      0      0      1         \"tea bad\"\t\n",
      "Example #3      0      2      1      0      0         \"bear drink bear\"\t\n",
      "Example #4      0      1      1      0      1         \"bear drink tea\"\t\n",
      "Example #5      0      1      0      1      0         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "#import operator\n",
    "#sorted_voc = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "sorted_voc = vectorizer.get_feature_names()\n",
    "print('\\t\\t', end = '')\n",
    "for j in range(len(vectorizer.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "    #print('{0:7s}'.format(sorted_voc[j][0]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_WF.shape[0]):\n",
    "        print('{0:7d}'.format(array_WF[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency method is used in order to reduce influence of a document length.\n",
    "\n",
    "The way to calculate it: $\\frac{\\rm Word ~ Frequency}{\\rm total ~ number ~ of ~ words ~ in ~ the ~ document}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         0.5        0.         0.5        0.        ]\n",
      " [0.5        0.         0.         0.         0.5       ]\n",
      " [0.         0.66666667 0.33333333 0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.         0.33333333]\n",
      " [0.         0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "array_TF = array_WF/array_WF.sum(axis=1,keepdims=True)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(array_TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the same results using `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         0.5        0.         0.5        0.        ]\n",
      " [0.5        0.         0.         0.         0.5       ]\n",
      " [0.         0.66666667 0.33333333 0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.         0.33333333]\n",
      " [0.         0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(use_idf=False, norm=\"l1\")\n",
    "array_TF1 = vectorizer2.fit_transform(corpus_clean).toarray()\n",
    "print(vectorizer2.get_feature_names())\n",
    "print( array_TF1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"term frequency\" (TF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t     bad    bear   drink  love   tea    \t\n",
      "Example #1   0.00   0.50   0.00   0.50   0.00         \"bear love\"\t\n",
      "Example #2   0.50   0.00   0.00   0.00   0.50         \"tea bad\"\t\n",
      "Example #3   0.00   0.67   0.33   0.00   0.00         \"bear drink bear\"\t\n",
      "Example #4   0.00   0.33   0.33   0.00   0.33         \"bear drink tea\"\t\n",
      "Example #5   0.00   0.50   0.00   0.50   0.00         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "sorted_voc = vectorizer2.get_feature_names()\n",
    "print('\\t     ', end = '')\n",
    "for j in range(len(vectorizer2.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer2.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_TF1.shape[0]):\n",
    "        print('{0:7.2f}'.format(array_TF1[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequencyâ€“Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The formula that is used to compute the $\\mbox{tf-idf }$ of term $t$ is\n",
    "\n",
    "$\\mbox{tf-idf}(d, t) = \\mbox{tf}(t) \\cdot \\mbox{idf}(d, t)$\n",
    "\n",
    "There are a number of ways to calculate $\\mbox{tf}$ and $\\mbox{idf}$. According to `TfidfVectorizer` documentation\n",
    "\n",
    "$\\mbox{tf}(t)$ here is word frequency,\n",
    "\n",
    "if `smooth_idf=False`, \n",
    "$\\mbox{idf}$ is computed as $\\mbox{idf}(d, t) = \\log \\left[ \\frac{n}{{\\rm df}(d, t)} \\right] + 1$,\n",
    "\n",
    "if `smooth_idf=True`, \n",
    "$\\mbox{idf}$ is computed as $\\mbox{idf}(d, t) = \\log \\left[ \\frac{ 1+n }{ 1+{\\rm df}(d, t) } \\right] + 1$,\n",
    "\n",
    "where $n$ is the total number of documents and $\\mbox{df}(d, t)$ is the document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         1.18232156 0.         1.69314718 0.        ]\n",
      " [2.09861229 0.         0.         0.         1.69314718]\n",
      " [0.         2.36464311 1.69314718 0.         0.        ]\n",
      " [0.         1.18232156 1.69314718 0.         1.69314718]\n",
      " [0.         1.18232156 0.         1.69314718 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "array_TFIDF = vectorizer3.fit_transform(corpus_clean).toarray()\n",
    "print( vectorizer3.get_feature_names() )\n",
    "print( array_TFIDF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"term frequency - inverse document frequency\" (TF-IDF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t     bad    bear   drink  love   tea    \t\n",
      "Example #1   0.00   1.18   0.00   1.69   0.00         \"bear love\"\t\n",
      "Example #2   2.10   0.00   0.00   0.00   1.69         \"tea bad\"\t\n",
      "Example #3   0.00   2.36   1.69   0.00   0.00         \"bear drink bear\"\t\n",
      "Example #4   0.00   1.18   1.69   0.00   1.69         \"bear drink tea\"\t\n",
      "Example #5   0.00   1.18   0.00   1.69   0.00         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "sorted_voc = vectorizer3.get_feature_names()\n",
    "print('\\t     ', end = '')\n",
    "for j in range(len(vectorizer3.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer3.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_TFIDF.shape[0]):\n",
    "        print('{0:7.2f}'.format(array_TFIDF[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "\n",
    "Here we use `Word2Vec` as example.\n",
    "There are a lot of ways to use the word embedding as features, here we use joining (averaging) vectors from the words from sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 16:31:44,181 [26252] WARNING  gensim.models.base_any2vec:1386: [JupyterRequire] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [sentence.split() for sentence in corpus_clean]\n",
    "model = word2vec.Word2Vec(tokenized_sentences, size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 16:31:51,499 [26252] WARNING  py.warnings:110: [JupyterRequire] C:\\Users\\OLEKSANDRRomanko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00201971,  0.0044131 , -0.00188085,  0.00446221, -0.00101235,\n",
       "        0.00336833,  0.00483887, -0.00391704, -0.00100387, -0.00132857,\n",
       "        0.00479649, -0.0023378 , -0.00184719,  0.00059363, -0.00248506,\n",
       "       -0.00480535, -0.00199169, -0.00101469, -0.00466638, -0.00298863,\n",
       "        0.00348795, -0.00208034,  0.00323432,  0.00089631, -0.00158379,\n",
       "        0.00470077, -0.00437122, -0.00127288,  0.00344179,  0.00313251,\n",
       "       -0.00368983,  0.00277271,  0.00407635,  0.00464457, -0.00481355,\n",
       "        0.00304931,  0.00108448,  0.00011703, -0.00156844, -0.00179355,\n",
       "        0.00135169, -0.00398491, -0.00296204, -0.00027267,  0.00409198,\n",
       "       -0.00015581, -0.00379952,  0.00221578,  0.0015273 ,  0.00309993,\n",
       "        0.00468623, -0.00387594, -0.00100491, -0.00405672, -0.00273765,\n",
       "       -0.00189837,  0.00187329, -0.00186427,  0.00172515,  0.00349746,\n",
       "       -0.00321778, -0.00095087,  0.00438536,  0.00034652,  0.00437058,\n",
       "        0.00460702,  0.00441507, -0.00016692, -0.00429769, -0.00107351,\n",
       "       -0.00406411, -0.00405155, -0.00399141, -0.00391832, -0.00292911,\n",
       "        0.00355493,  0.00353093,  0.00245617, -0.00233953,  0.00473853,\n",
       "        0.00040093,  0.00417835,  0.00341942,  0.00300494, -0.0041518 ,\n",
       "        0.0008761 , -0.00459361, -0.00184087, -0.00332112,  0.00065326,\n",
       "        0.00395293, -0.00424266,  0.00249247,  0.00297013,  0.00021573,\n",
       "        0.00125919,  0.0043053 ,  0.00461732, -0.00282668,  0.00259601],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 16:31:55,582 [26252] WARNING  py.warnings:110: [JupyterRequire] C:\\Users\\OLEKSANDRRomanko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tea', 0.04989343136548996),\n",
       " ('bear', -0.03948143869638443),\n",
       " ('bad', -0.04141325503587723),\n",
       " ('drink', -0.06769246608018875)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    text = text.split(' ')\n",
    "    for word in text:\n",
    "        vec += model[word].reshape((1, size))\n",
    "        count += 1.\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 16:32:03,598 [26252] WARNING  py.warnings:110: [JupyterRequire] C:\\Users\\OLEKSANDRRomanko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_wordEmbedding = np.concatenate([buildWordVector(z, 100) for z in corpus_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.41971167e-04  8.54137121e-04  1.17415149e-03  4.63191746e-03\n",
      "  -6.03307562e-05  3.10892600e-03  3.60774726e-03  3.79402656e-04\n",
      "   9.75768664e-04  5.12669038e-04  3.91771831e-03 -1.50340647e-03\n",
      "   1.08108588e-03 -6.18075137e-06 -2.76957871e-04 -3.28093220e-03\n",
      "  -1.82390388e-04  1.84900156e-03  1.20697077e-04 -9.53201263e-04\n",
      "   2.09853752e-05 -3.18862859e-03  1.17352116e-04  6.92134694e-04\n",
      "   1.05383032e-03  4.00270883e-03 -3.78950220e-03 -3.52097355e-04\n",
      "   3.03843187e-03  1.44166926e-03 -7.86845689e-04 -1.00596051e-03\n",
      "   1.44461612e-03  4.47982270e-03 -2.16282866e-03 -9.43378545e-05\n",
      "   1.94498088e-03  5.97301045e-04 -1.16750022e-03 -2.96385260e-05\n",
      "   8.67735580e-04 -1.90259558e-03  7.20671378e-04 -1.97848966e-03\n",
      "   2.38357892e-03 -2.53623130e-03 -4.39201796e-03 -3.58355232e-04\n",
      "  -1.50532206e-03  2.74961535e-03  4.13743942e-03  5.60019631e-04\n",
      "  -1.88610528e-03 -2.96315702e-03 -2.15398712e-03 -1.10591236e-03\n",
      "   2.45791767e-03  1.34875003e-03  1.08548875e-03  1.22183905e-03\n",
      "  -2.16459198e-03 -2.69498670e-03  3.10972653e-03  2.25075636e-03\n",
      "   1.16449804e-03  4.49925498e-03 -8.83825123e-06 -1.90643756e-03\n",
      "  -6.98774471e-04  1.63412426e-03 -2.76352133e-03 -2.53803073e-03\n",
      "  -4.38644877e-03 -2.46869167e-05 -3.90815747e-03 -4.47115628e-04\n",
      "   1.89061374e-03 -1.09374837e-03  8.48564552e-04  2.04136851e-03\n",
      "   2.04285771e-03  2.59749254e-03 -2.98995408e-04 -2.97199702e-04\n",
      "  -2.95000628e-03 -7.73639826e-04 -1.60265947e-04  1.51599932e-04\n",
      "  -6.04518456e-04  2.62023989e-03  4.31342539e-03 -1.59109978e-03\n",
      "   4.45073762e-04 -7.89313344e-04  1.99092235e-03  3.03631532e-03\n",
      "   2.69174535e-03  3.69780301e-03 -2.82608694e-03  2.44110764e-03]\n",
      " [-1.47880678e-03  3.29768518e-05 -4.04789345e-04  1.18474389e-03\n",
      "  -8.96025856e-04  2.23750307e-03  2.78302527e-04  2.35688675e-03\n",
      "  -1.10495667e-03 -8.55235616e-04  4.30969521e-07 -2.56631151e-03\n",
      "  -1.63220038e-03  1.63280670e-03  6.70992122e-04  2.85808611e-03\n",
      "   1.01616323e-03  4.84093209e-04 -1.06297340e-03 -2.88769044e-03\n",
      "   3.98087199e-04  1.03955535e-03 -2.91457551e-03 -2.39158758e-03\n",
      "   5.53604215e-04  7.31227396e-04 -1.28248264e-03  2.78311549e-04\n",
      "  -6.92435890e-04 -3.06819309e-03  3.01215024e-03  2.95221398e-04\n",
      "  -2.46730051e-04 -2.21929323e-04  3.13759258e-03  5.12271130e-04\n",
      "   1.77139719e-03 -1.70199617e-03  3.16609268e-03 -1.35657378e-04\n",
      "  -2.62580288e-04  1.82195799e-04 -1.19874661e-03 -1.23493749e-03\n",
      "   2.79526704e-03  1.05215737e-03 -3.13435716e-03 -2.11051467e-03\n",
      "   1.69490161e-03  5.10771526e-04 -8.98816506e-04 -2.20530329e-03\n",
      "  -8.83566703e-04  3.31627449e-03 -2.79253814e-04  3.61834897e-03\n",
      "   4.00065060e-03  8.85813264e-04  8.62907385e-04 -5.02343057e-04\n",
      "  -4.83986805e-03 -6.81283767e-04 -2.23007891e-03  9.68828186e-04\n",
      "  -3.65959457e-03  1.09104440e-03 -4.49312688e-03 -3.74846766e-03\n",
      "  -2.87131639e-03 -1.34794455e-03 -1.59885350e-03 -6.60270336e-04\n",
      "   1.03242893e-03  3.23157047e-03 -1.36360846e-03  3.01467325e-03\n",
      "   7.29931984e-04 -4.64009470e-04  3.82553367e-03  1.60332664e-03\n",
      "   3.37341364e-04 -7.59590417e-04  3.10917082e-03  2.46526801e-03\n",
      "  -2.67259308e-03  2.11120129e-03 -1.70386629e-05 -2.85559698e-03\n",
      "   2.94042798e-03  1.35389459e-03  3.00607714e-03 -4.31258441e-03\n",
      "  -1.09124044e-03  4.84604388e-04 -4.49293735e-03  2.25881889e-03\n",
      "  -1.54926882e-03 -3.17666959e-03  3.61864711e-03  1.83933211e-04]\n",
      " [-1.68681129e-03 -3.44023962e-03  1.97593076e-03  4.71388440e-03\n",
      "   4.47768330e-04  2.44074583e-03  1.35032615e-04  2.37072143e-03\n",
      "   2.77209181e-03  1.86543460e-03  3.42299913e-03 -1.13519075e-03\n",
      "   3.91970870e-03 -1.68343183e-03  7.91133847e-04 -1.22378787e-03\n",
      "   2.69774526e-03  4.66248843e-03  3.28774470e-03  1.63396331e-03\n",
      "  -1.39587993e-03 -1.70298386e-03 -2.21793957e-03  1.61517978e-03\n",
      "   2.95560054e-03  1.49664904e-03 -2.46068319e-03  1.50078190e-03\n",
      "   3.38232502e-03 -1.06610069e-03  2.37357764e-03 -4.45691178e-03\n",
      "   3.64755668e-04  4.07834491e-03 -3.35065997e-04 -3.13034382e-03\n",
      "   2.27298525e-03  1.99838821e-03  6.09315699e-04 -3.44820631e-05\n",
      "  -3.01680819e-04 -5.10241303e-04  1.46369264e-03 -2.25635542e-03\n",
      "   4.21730025e-05 -3.85448961e-03 -3.34237610e-03 -3.74900022e-04\n",
      "  -4.36039362e-03  1.52484611e-03  2.09605211e-03  2.39823270e-03\n",
      "  -3.28438838e-03 -1.30643602e-04  5.38236384e-04  1.19480555e-03\n",
      "   4.87757847e-04  3.33926499e-03  8.64121073e-04  3.21423635e-04\n",
      "  -1.23039796e-03 -4.61971915e-03  2.66561634e-03  2.41736567e-03\n",
      "  -2.87074968e-03  1.43276000e-03 -3.81869140e-03 -1.04422246e-03\n",
      "   2.34104460e-03  4.22322657e-03 -2.27862294e-03  5.65517616e-04\n",
      "  -1.91843805e-03  2.49888796e-03 -4.78536931e-03 -1.67569596e-03\n",
      "  -7.30286896e-05 -4.73308051e-03  3.56595567e-03 -7.17438137e-04\n",
      "   2.73243012e-03 -2.60482465e-04 -3.23317945e-03 -3.28490751e-03\n",
      "  -2.16288081e-03 -2.86576245e-03  1.42995641e-03  9.33180563e-05\n",
      "   1.51089835e-03  2.38234992e-03  3.25119316e-03  2.14187258e-03\n",
      "  -2.26204594e-04 -2.72331951e-03  9.83206555e-04  2.73927526e-03\n",
      "  -8.12386085e-04  1.56462055e-03 -2.58398840e-03  1.77294804e-03]\n",
      " [-2.58515965e-03 -1.33757852e-03  1.76707236e-03  3.78395353e-03\n",
      "   6.52869989e-04  2.46984888e-03 -1.09369614e-03  1.60822777e-03\n",
      "   1.40004888e-03  1.94335603e-03  8.72297368e-04 -1.69733355e-03\n",
      "   1.16721354e-03 -1.19423822e-03  7.64440386e-05  9.43673959e-04\n",
      "   2.16131969e-03  4.67085590e-03  6.37263978e-04  1.03620308e-04\n",
      "   2.16417635e-04  3.20932207e-05 -2.05267071e-03  1.39370263e-03\n",
      "   9.03576380e-04  5.33779889e-04 -1.85261919e-03  4.81432944e-04\n",
      "   2.80888953e-03 -2.03402017e-03  3.15963880e-03 -3.05411002e-03\n",
      "  -4.59891201e-04  2.64581694e-03  1.07725650e-03 -1.47573852e-03\n",
      "   1.37198616e-03  4.73537327e-04  2.18075948e-03 -1.58134305e-04\n",
      "   7.28984887e-05 -2.11325284e-03 -1.62856467e-03 -2.10740675e-03\n",
      "   4.77427850e-04 -2.63729525e-03 -2.23622061e-03 -3.61190178e-06\n",
      "  -3.18866642e-03  1.88229369e-03  1.82373750e-03  3.25540391e-04\n",
      "  -2.42117642e-03  1.41829338e-03  1.72855096e-03  2.90886112e-03\n",
      "   4.79532794e-04  2.93251428e-03  2.25112462e-03 -2.01624081e-04\n",
      "  -2.43719004e-03 -4.14916687e-03  1.74896920e-03  1.53019372e-03\n",
      "  -3.71834356e-03  1.32618643e-03 -3.71266250e-03 -6.82096540e-04\n",
      "   2.38931272e-04  2.09959045e-03 -3.21033221e-03  5.52204438e-05\n",
      "  -1.05479833e-03  1.92391036e-03 -3.61039020e-03  8.43205179e-04\n",
      "   6.65659560e-04 -2.19922171e-03  3.21810188e-03 -3.17973318e-04\n",
      "   1.32480995e-03 -1.44991667e-03 -1.06918944e-03 -4.83566818e-04\n",
      "  -1.87774730e-03 -1.43295565e-03 -7.32709964e-04 -2.24003724e-03\n",
      "   2.41650230e-03  1.68842007e-03  2.40160780e-03  4.43875246e-04\n",
      "  -1.09766109e-03 -2.51659757e-03 -1.91350708e-03  2.19386149e-03\n",
      "  -1.14582947e-03 -3.18407527e-04 -1.06422541e-04  8.24551370e-04]\n",
      " [ 2.41971167e-04  8.54137121e-04  1.17415149e-03  4.63191746e-03\n",
      "  -6.03307562e-05  3.10892600e-03  3.60774726e-03  3.79402656e-04\n",
      "   9.75768664e-04  5.12669038e-04  3.91771831e-03 -1.50340647e-03\n",
      "   1.08108588e-03 -6.18075137e-06 -2.76957871e-04 -3.28093220e-03\n",
      "  -1.82390388e-04  1.84900156e-03  1.20697077e-04 -9.53201263e-04\n",
      "   2.09853752e-05 -3.18862859e-03  1.17352116e-04  6.92134694e-04\n",
      "   1.05383032e-03  4.00270883e-03 -3.78950220e-03 -3.52097355e-04\n",
      "   3.03843187e-03  1.44166926e-03 -7.86845689e-04 -1.00596051e-03\n",
      "   1.44461612e-03  4.47982270e-03 -2.16282866e-03 -9.43378545e-05\n",
      "   1.94498088e-03  5.97301045e-04 -1.16750022e-03 -2.96385260e-05\n",
      "   8.67735580e-04 -1.90259558e-03  7.20671378e-04 -1.97848966e-03\n",
      "   2.38357892e-03 -2.53623130e-03 -4.39201796e-03 -3.58355232e-04\n",
      "  -1.50532206e-03  2.74961535e-03  4.13743942e-03  5.60019631e-04\n",
      "  -1.88610528e-03 -2.96315702e-03 -2.15398712e-03 -1.10591236e-03\n",
      "   2.45791767e-03  1.34875003e-03  1.08548875e-03  1.22183905e-03\n",
      "  -2.16459198e-03 -2.69498670e-03  3.10972653e-03  2.25075636e-03\n",
      "   1.16449804e-03  4.49925498e-03 -8.83825123e-06 -1.90643756e-03\n",
      "  -6.98774471e-04  1.63412426e-03 -2.76352133e-03 -2.53803073e-03\n",
      "  -4.38644877e-03 -2.46869167e-05 -3.90815747e-03 -4.47115628e-04\n",
      "   1.89061374e-03 -1.09374837e-03  8.48564552e-04  2.04136851e-03\n",
      "   2.04285771e-03  2.59749254e-03 -2.98995408e-04 -2.97199702e-04\n",
      "  -2.95000628e-03 -7.73639826e-04 -1.60265947e-04  1.51599932e-04\n",
      "  -6.04518456e-04  2.62023989e-03  4.31342539e-03 -1.59109978e-03\n",
      "   4.45073762e-04 -7.89313344e-04  1.99092235e-03  3.03631532e-03\n",
      "   2.69174535e-03  3.69780301e-03 -2.82608694e-03  2.44110764e-03]]\n"
     ]
    }
   ],
   "source": [
    "print( array_wordEmbedding )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
